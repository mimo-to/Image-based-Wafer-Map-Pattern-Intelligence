{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_md"
            },
            "source": [
                "# WaferScan AI: Data Exploration & Processing\n",
                "\n",
                "This notebook performs the initial data pipeline:\n",
                "1.  **Loading & Cleaning**: Parsing raw WM-811K data.\n",
                "2.  **Imbalance Analysis**: Quantifying class distribution.\n",
                "3.  **Visualization**: Inspecting sample defects.\n",
                "4.  **Stratified Split**: Creating reproducible Train/Val/Test sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup_import",
                "jupyter": {
                    "source_hidden": true
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter\n",
                "from tqdm import tqdm\n",
                "from google.colab import drive\n",
                "\n",
                "# Mount Drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Define Paths\n",
                "PROJECT_ROOT = \"/content/drive/MyDrive/wafer-hackathon\"\n",
                "RAW_DATA_PATH = f\"{PROJECT_ROOT}/data/raw/LSWMD.pkl\"\n",
                "PROCESSED_PATH = f\"{PROJECT_ROOT}/data/processed\"\n",
                "DOCS_PATH = f\"{PROJECT_ROOT}/docs\"\n",
                "\n",
                "# Ensure directories exist\n",
                "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
                "os.makedirs(DOCS_PATH, exist_ok=True)\n",
                "\n",
                "print(\"Environment Setup Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "preproc_md"
            },
            "source": [
                "## Data Processing Pipeline (Raw -> Processed)\n",
                "\n",
                "Helper function to load and clean the legacy pickle file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b20b0d60",
            "metadata": {
                "id": "loader_func"
            },
            "outputs": [],
            "source": [
                "# Legacy pandas compatibility\n",
                "if \"pandas.indexes\" not in sys.modules:\n",
                "    sys.modules[\"pandas.indexes\"] = pd.core.indexes\n",
                "\n",
                "def load_wafer_dataset(pkl_path):\n",
                "    \"\"\"\n",
                "    Load and clean WM-811K dataset from pickle.\n",
                "    Extracted from actual_01_data_exploration.ipynb.\n",
                "    \"\"\"\n",
                "    if not os.path.exists(pkl_path):\n",
                "        raise FileNotFoundError(f\"File not found: {pkl_path}\")\n",
                "    \n",
                "    print(f\"Loading raw data from {pkl_path}...\")\n",
                "    try:\n",
                "        with open(pkl_path, \"rb\") as f:\n",
                "            data = pickle.load(f, encoding=\"latin1\")\n",
                "    except Exception as e:\n",
                "        raise ValueError(f\"Pickle load failed: {e}\")\n",
                "\n",
                "    # Class mapping\n",
                "    class_map = {\n",
                "        \"none\": 0, \"Center\": 1, \"Donut\": 2, \"Edge-Loc\": 3, \n",
                "        \"Edge-Ring\": 4, \"Loc\": 5, \"Near-full\": 6, \"Random\": 7, \"Scratch\": 8\n",
                "    }\n",
                "\n",
                "    images = []\n",
                "    labels = []\n",
                "    metadata_rows = []\n",
                "    skipped = 0\n",
                "\n",
                "    iterator = data.itertuples() if isinstance(data, pd.DataFrame) else data\n",
                "    total = len(data)\n",
                "\n",
                "    for record in tqdm(iterator, total=total, desc=\"Processing\"):\n",
                "        try:\n",
                "            # Extract fields robustly\n",
                "            if isinstance(record, tuple) and hasattr(record, \"waferMap\"):\n",
                "                wafer_map = record.waferMap\n",
                "                failure_type = record.failureType\n",
                "                lot_name = getattr(record, \"lotName\", \"\")\n",
                "                wafer_index = getattr(record, \"waferIndex\", 0)\n",
                "            elif isinstance(record, dict):\n",
                "                wafer_map = record.get(\"waferMap\")\n",
                "                failure_type = record.get(\"failureType\")\n",
                "                lot_name = record.get(\"lotName\", \"\")\n",
                "                wafer_index = record.get(\"waferIndex\", 0)\n",
                "            else:\n",
                "                wafer_map = getattr(record, \"waferMap\", None)\n",
                "                failure_type = getattr(record, \"failureType\", None)\n",
                "                lot_name = getattr(record, \"lotName\", \"\")\n",
                "                wafer_index = getattr(record, \"waferIndex\", 0)\n",
                "\n",
                "            # Validate\n",
                "            if wafer_map is None or failure_type is None:\n",
                "                skipped += 1\n",
                "                continue\n",
                "            \n",
                "            wafer_map = np.array(wafer_map)\n",
                "            if wafer_map.ndim != 2:\n",
                "                skipped += 1\n",
                "                continue\n",
                "\n",
                "            # Normalize label\n",
                "            if isinstance(failure_type, (np.ndarray, list)):\n",
                "                if len(failure_type) == 0:\n",
                "                    f_label = \"none\"\n",
                "                else:\n",
                "                    item = failure_type[0] if isinstance(failure_type, list) else failure_type.flat[0]\n",
                "                    f_label = str(item[0]) if isinstance(item, list) and len(item) > 0 else str(item)\n",
                "            elif isinstance(failure_type, str):\n",
                "                f_label = failure_type\n",
                "            else:\n",
                "                skipped += 1\n",
                "                continue\n",
                "            \n",
                "            f_label = f_label.strip()\n",
                "            if f_label not in class_map:\n",
                "                skipped += 1\n",
                "                continue\n",
                "\n",
                "            label_idx = class_map[f_label]\n",
                "            images.append(wafer_map)\n",
                "            labels.append(label_idx)\n",
                "            metadata_rows.append({\n",
                "                \"lotName\": lot_name,\n",
                "                \"waferIndex\": wafer_index,\n",
                "                \"failureType\": f_label,\n",
                "                \"mapped_label\": label_idx\n",
                "            })\n",
                "\n",
                "        except Exception:\n",
                "            skipped += 1\n",
                "            continue\n",
                "\n",
                "    print(f\"Loaded {len(images)} valid samples. Skipped: {skipped}\")\n",
                "    return images, labels, pd.DataFrame(metadata_rows)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87a4f6ee",
            "metadata": {
                "id": "save_intermediate"
            },
            "outputs": [],
            "source": [
                "# Execute Pipeline if processed file missing or needs update\n",
                "# (Always runs to ensure latest logic)\n",
                "images, labels, metadata = load_wafer_dataset(RAW_DATA_PATH)\n",
                "\n",
                "# Save Intermediate\n",
                "save_path = f\"{PROCESSED_PATH}/processed_dataset.pkl\"\n",
                "with open(save_path, 'wb') as f:\n",
                "    pickle.dump({\n",
                "        'images': images,\n",
                "        'labels': labels,\n",
                "        'metadata': metadata\n",
                "    }, f)\n",
                "\n",
                "print(f\"Intermediate data saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f93cd30b",
            "metadata": {
                "id": "sec1_md"
            },
            "source": [
                "### SECTION 1: Load Data\n",
                "- Load from processed_dataset.pkl (saved in previous step)\n",
                "- Verify data loaded correctly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "201f25fc",
            "metadata": {
                "id": "sec1_code"
            },
            "outputs": [],
            "source": [
                "load_path = f\"{PROCESSED_PATH}/processed_dataset.pkl\"\n",
                "print(f\"Loading from {load_path}...\")\n",
                "\n",
                "with open(load_path, 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "\n",
                "loaded_images = data['images']\n",
                "loaded_labels = data['labels']\n",
                "loaded_meta = data['metadata']\n",
                "\n",
                "print(\"Data loaded successfully.\")\n",
                "print(f\"Total Samples: {len(loaded_images)}\")\n",
                "print(f\"Labels Count: {len(loaded_labels)}\")\n",
                "print(f\"Metadata Shape: {loaded_meta.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b49ad9f2",
            "metadata": {
                "id": "sec2_md"
            },
            "source": [
                "### SECTION 2: Class Distribution\n",
                "- Create bar chart showing samples per class\n",
                "- Calculate and display imbalance ratio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "920413b3",
            "metadata": {
                "id": "sec2_code"
            },
            "outputs": [],
            "source": [
                "label_counts = Counter(loaded_labels)\n",
                "sorted_counts = dict(sorted(label_counts.items()))\n",
                "\n",
                "class_names = {\n",
                "    0: 'none', 1: 'Center', 2: 'Donut', 3: 'Edge-Loc', \n",
                "    4: 'Edge-Ring', 5: 'Loc', 6: 'Near-full', 7: 'Random', 8: 'Scratch'\n",
                "}\n",
                "\n",
                "x = [class_names[k] for k in sorted_counts.keys()]\n",
                "y = list(sorted_counts.values())\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "bars = plt.bar(x, y, color='skyblue', edgecolor='black')\n",
                "plt.title('Class Distribution (WM-811K)')\n",
                "plt.xlabel('Failure Type')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=45)\n",
                "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "\n",
                "# Add labels\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:,}', \n",
                "             ha='center', va='bottom', fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "save_fig_path = f\"{DOCS_PATH}/class_distribution.png\"\n",
                "plt.savefig(save_fig_path, dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Figure saved: {save_fig_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2fbb349e",
            "metadata": {
                "id": "sec3_md"
            },
            "source": [
                "### SECTION 3: Sample Visualization (3Ã—3 grid)\n",
                "- Show one example from each of the 9 defect classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7338e1d7",
            "metadata": {
                "id": "sec3_code"
            },
            "outputs": [],
            "source": [
                "# Find one index per class\n",
                "sample_indices = {}\n",
                "for idx, label in enumerate(loaded_labels):\n",
                "    if label not in sample_indices:\n",
                "        sample_indices[label] = idx\n",
                "    if len(sample_indices) == 9:\n",
                "        break\n",
                "\n",
                "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for class_id in range(9):\n",
                "    ax = axes[class_id]\n",
                "    idx = sample_indices.get(class_id)\n",
                "    if idx is not None:\n",
                "        img = loaded_images[idx]\n",
                "        ax.imshow(img, cmap='gray')\n",
                "        ax.set_title(class_names[class_id])\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "save_sample_path = f\"{DOCS_PATH}/sample_defects.png\"\n",
                "plt.savefig(save_sample_path, dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Figure saved: {save_sample_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fe848944",
            "metadata": {
                "id": "sec4_md"
            },
            "source": [
                "### SECTION 4: Class Imbalance Analysis\n",
                "- Print imbalance ratio\n",
                "- Note on class weighting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c0d4ccd",
            "metadata": {
                "id": "sec4_code"
            },
            "outputs": [],
            "source": [
                "defect_counts = [v for k, v in sorted_counts.items() if k != 0]\n",
                "if defect_counts:\n",
                "    max_count = max(defect_counts)\n",
                "    min_count = min(defect_counts)\n",
                "    ratio = max_count / min_count\n",
                "    print(f\"Max Defect Count: {max_count}\")\n",
                "    print(f\"Min Defect Count: {min_count}\")\n",
                "    print(f\"Imbalance Ratio: {ratio:.2f}\")\n",
                "    print(\"NOTE: This requires class weighting during training to handle the imbalance.\")\n",
                "else:\n",
                "    print(\"No defects found in loaded subset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb91d08d",
            "metadata": {
                "id": "sec5_md"
            },
            "source": [
                "### SECTION 5: Image Statistics\n",
                "- Sample 1000 random images\n",
                "- Count dimension occurrences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fef0401",
            "metadata": {
                "id": "sec5_code"
            },
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "num_samples = min(1000, len(loaded_images))\n",
                "indices = np.random.choice(len(loaded_images), num_samples, replace=False)\n",
                "\n",
                "sizes = [(loaded_images[i].shape[0], loaded_images[i].shape[1]) for i in indices]\n",
                "size_counts = Counter(sizes)\n",
                "\n",
                "print(f\"Analyzed {num_samples} random variants.\")\n",
                "print(f\"Total unique sizes found: {len(size_counts)}\")\n",
                "print(\"Top 5 most common sizes (H, W):\")\n",
                "for size, count in size_counts.most_common(5):\n",
                "    print(f\"  {size}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0f4ca6d4",
            "metadata": {
                "id": "split_md"
            },
            "source": [
                "### SECTION 6: Stratified Split\n",
                "- Create Train (70%) / Val (15%) / Test (15%) splits\n",
                "- Save indices to processed_dataset.pkl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "446b775f",
            "metadata": {
                "id": "split_code"
            },
            "outputs": [],
            "source": [
                "from sklearn.model_selection import StratifiedShuffleSplit\n",
                "\n",
                "all_labels = np.array(loaded_labels)\n",
                "all_indices = np.arange(len(all_labels))\n",
                "\n",
                "# 1. Split Train (70%) vs Temp (30%)\n",
                "# Using actual_01_data_exploration logic\n",
                "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
                "train_idx, temp_idx = next(sss1.split(all_indices, all_labels))\n",
                "\n",
                "# 2. Split Temp into Val (50% of 30% = 15%) and Test (50% of 30% = 15%)\n",
                "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
                "val_sub_idx, test_sub_idx = next(sss2.split(temp_idx, all_labels[temp_idx]))\n",
                "\n",
                "val_idx = temp_idx[val_sub_idx]\n",
                "test_idx = temp_idx[test_sub_idx]\n",
                "\n",
                "print(f\"Train size: {len(train_idx)} ({len(train_idx)/len(all_labels):.1%})\")\n",
                "print(f\"Val size:   {len(val_idx)} ({len(val_idx)/len(all_labels):.1%})\")\n",
                "print(f\"Test size:  {len(test_idx)} ({len(test_idx)/len(all_labels):.1%})\")\n",
                "\n",
                "# Update Pickle\n",
                "with open(load_path, 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "\n",
                "data['train_indices'] = train_idx\n",
                "data['val_indices'] = val_idx\n",
                "data['test_indices'] = test_idx\n",
                "\n",
                "with open(load_path, 'wb') as f:\n",
                "    pickle.dump(data, f)\n",
                "\n",
                "print(f\"Processed dataset updated with stratified splits at: {load_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
