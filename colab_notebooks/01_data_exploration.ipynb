{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ecf241c4",
            "metadata": {
                "id": "intro_md"
            },
            "source": [
                "# WaferScan AI: Data Processing & Exploration\n",
                "\n",
                "This notebook handles the initial data pipeline for the WM-811K dataset. It performs:\n",
                "1.  **Loading & Cleaning**: Parsing the raw pickle file and removing corrupted entries.\n",
                "2.  **Label Mapping**: Converting string defect types to integer class IDs (0-8).\n",
                "3.  **Visualization**: Inspecting class balance and sample defects.\n",
                "4.  **Stratified Split**: Creating reproducible Train/Val/Test sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45ae63c2",
            "metadata": {
                "id": "drive_mount"
            },
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6dd8537b",
            "metadata": {
                "id": "setup_md"
            },
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47c52e63",
            "metadata": {
                "id": "dir_setup"
            },
            "outputs": [],
            "source": [
                "PROJECT_ROOT = \"/content/drive/MyDrive/wafer-hackathon\"\n",
                "\n",
                "!mkdir -p $PROJECT_ROOT/data/raw\n",
                "!mkdir -p $PROJECT_ROOT/data/processed\n",
                "!mkdir -p $PROJECT_ROOT/models/checkpoints\n",
                "!mkdir -p $PROJECT_ROOT/models/final\n",
                "!mkdir -p $PROJECT_ROOT/models/onnx\n",
                "!mkdir -p $PROJECT_ROOT/models/metrics\n",
                "!mkdir -p $PROJECT_ROOT/docs\n",
                "\n",
                "print(\"Project structure verified.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62983221",
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install numpy==1.26.4 timm==0.9.12 netcal==1.3.5 wandb==0.16.1 pandas==2.0.3"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "04355217",
            "metadata": {
                "id": "loader_md"
            },
            "source": [
                "## 2. Data Loader Implementation\n",
                "\n",
                "Handles legacy pandas compatibility and robustly parses the nested pickle structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d175feb",
            "metadata": {
                "id": "loader_code"
            },
            "outputs": [],
            "source": [
                "import pickle\n",
                "import os\n",
                "import sys\n",
                "from collections import Counter\n",
                "from typing import List, Tuple, Dict, Any\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Legacy pandas pickle support\n",
                "if \"pandas.indexes\" not in sys.modules:\n",
                "    sys.modules[\"pandas.indexes\"] = pd.core.indexes\n",
                "\n",
                "\n",
                "def load_wafer_dataset(\n",
                "    pkl_path: str,\n",
                ") -> Tuple[List[np.ndarray], List[int], pd.DataFrame]:\n",
                "    \"\"\"\n",
                "    Load and validate WM-811K dataset from pickle file.\n",
                "    \"\"\"\n",
                "    if not os.path.exists(pkl_path):\n",
                "        raise FileNotFoundError(f\"Dataset file not found: {pkl_path}\")\n",
                "\n",
                "    print(f\"Loading dataset from {pkl_path}...\")\n",
                "    try:\n",
                "        with open(pkl_path, \"rb\") as f:\n",
                "            data = pickle.load(f, encoding=\"latin1\")\n",
                "    except Exception as e:\n",
                "        raise ValueError(f\"Failed to load pickle: {e}\")\n",
                "\n",
                "    # Define class mapping\n",
                "    class_map = {\n",
                "        \"none\": 0,\n",
                "        \"Center\": 1,\n",
                "        \"Donut\": 2,\n",
                "        \"Edge-Loc\": 3,\n",
                "        \"Edge-Ring\": 4,\n",
                "        \"Loc\": 5,\n",
                "        \"Near-full\": 6,\n",
                "        \"Random\": 7,\n",
                "        \"Scratch\": 8,\n",
                "    }\n",
                "\n",
                "    images: List[np.ndarray] = []\n",
                "    labels: List[int] = []\n",
                "    metadata_rows: List[Dict[str, Any]] = []\n",
                "\n",
                "    skipped_count = 0\n",
                "    iterator = data\n",
                "    total = len(data)\n",
                "\n",
                "    if isinstance(data, pd.DataFrame):\n",
                "        iterator = data.itertuples()\n",
                "\n",
                "    for record in tqdm(iterator, total=total, desc=\"Processing wafers\"):\n",
                "        try:\n",
                "            if isinstance(record, tuple) and hasattr(record, \"waferMap\"):\n",
                "                wafer_map = record.waferMap\n",
                "                failure_type = record.failureType\n",
                "                lot_name = getattr(record, \"lotName\", \"\")\n",
                "                wafer_index = getattr(record, \"waferIndex\", 0)\n",
                "            elif isinstance(record, dict):\n",
                "                wafer_map = record.get(\"waferMap\")\n",
                "                failure_type = record.get(\"failureType\")\n",
                "                lot_name = record.get(\"lotName\", \"\")\n",
                "                wafer_index = record.get(\"waferIndex\", 0)\n",
                "            else:\n",
                "                wafer_map = getattr(record, \"waferMap\", None)\n",
                "                failure_type = getattr(record, \"failureType\", None)\n",
                "                lot_name = getattr(record, \"lotName\", \"\")\n",
                "                wafer_index = getattr(record, \"waferIndex\", 0)\n",
                "\n",
                "            if wafer_map is None or failure_type is None:\n",
                "                skipped_count += 1\n",
                "                continue\n",
                "\n",
                "            wafer_map = np.array(wafer_map)\n",
                "            if wafer_map.ndim != 2:\n",
                "                skipped_count += 1\n",
                "                continue\n",
                "\n",
                "            if isinstance(failure_type, np.ndarray):\n",
                "                if failure_type.size == 0:\n",
                "                    f_label = \"none\"\n",
                "                else:\n",
                "                    item = failure_type.flat[0]\n",
                "                    f_label = str(item)\n",
                "            elif isinstance(failure_type, list):\n",
                "                if len(failure_type) == 0:\n",
                "                    f_label = \"none\"\n",
                "                else:\n",
                "                    item = failure_type[0]\n",
                "                    if isinstance(item, list) and len(item) > 0:\n",
                "                        f_label = str(item[0])\n",
                "                    else:\n",
                "                        f_label = str(item)\n",
                "            elif isinstance(failure_type, str):\n",
                "                f_label = failure_type\n",
                "            else:\n",
                "                skipped_count += 1\n",
                "                continue\n",
                "\n",
                "            f_label = f_label.strip()\n",
                "            if f_label not in class_map:\n",
                "                skipped_count += 1\n",
                "                continue\n",
                "\n",
                "            label_idx = class_map[f_label]\n",
                "\n",
                "            images.append(wafer_map)\n",
                "            labels.append(label_idx)\n",
                "            metadata_rows.append(\n",
                "                {\n",
                "                    \"lotName\": lot_name,\n",
                "                    \"waferIndex\": wafer_index,\n",
                "                    \"failureType\": f_label,\n",
                "                    \"mapped_label\": label_idx,\n",
                "                }\n",
                "            )\n",
                "        except Exception:\n",
                "            skipped_count += 1\n",
                "            continue\n",
                "\n",
                "    metadata_df = pd.DataFrame(metadata_rows)\n",
                "    valid_count = len(images)\n",
                "\n",
                "    print(\"-\" * 40)\n",
                "    print(f\"Data Loading Complete: {valid_count} valid samples\")\n",
                "    print(f\"Skipped samples: {skipped_count}\")\n",
                "    print(\"-\" * 40)\n",
                "\n",
                "    if valid_count > 0:\n",
                "        heights = [img.shape[0] for img in images]\n",
                "        widths = [img.shape[1] for img in images]\n",
                "        print(f\"Image Heights: Min={min(heights)}, Max={max(heights)}\")\n",
                "        print(f\"Image Widths:  Min={min(widths)}, Max={max(widths)}\")\n",
                "\n",
                "    return images, labels, metadata_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e6f6ce47",
            "metadata": {
                "id": "exec_load_md"
            },
            "source": [
                "## 3. Execution: Load Raw Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0c9b411",
            "metadata": {
                "id": "exec_load"
            },
            "outputs": [],
            "source": [
                "images, labels, metadata = load_wafer_dataset(\n",
                "    f'{PROJECT_ROOT}/data/raw/LSWMD.pkl'\n",
                ")\n",
                "\n",
                "print(\"Loaded:\", len(images))\n",
                "print(\"Metadata shape:\", metadata.shape)\n",
                "print(\"Unique labels:\", sorted(set(labels)))\n",
                "print(\"Label distribution:\", Counter(labels))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "139f5fb6",
            "metadata": {
                "id": "save_md"
            },
            "source": [
                "## 4. Save Intermediate Processed Data\n",
                "\n",
                "Saves cleaned data before splitting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71f5b8c9",
            "metadata": {
                "id": "save_code"
            },
            "outputs": [],
            "source": [
                "processed_path = f'{PROJECT_ROOT}/data/processed'\n",
                "os.makedirs(processed_path, exist_ok=True)\n",
                "\n",
                "with open(f'{processed_path}/processed_dataset.pkl', 'wb') as f:\n",
                "    pickle.dump({\n",
                "        'images': images,\n",
                "        'labels': labels,\n",
                "        'metadata': metadata\n",
                "    }, f)\n",
                "\n",
                "print(f\"processed_dataset.pkl saved inside {processed_path}/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bffe3a3b",
            "metadata": {
                "id": "viz_md"
            },
            "source": [
                "## 5. Exploratory Visualization\n",
                "\n",
                "**Class Distribution**: Shows severe class imbalance (96% 'none').\n",
                "**Sample Defects**: Visual confirmation of label mapping."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ffaafe16",
            "metadata": {
                "id": "viz_code"
            },
            "outputs": [],
            "source": [
                "docs_path = f'{PROJECT_ROOT}/docs'\n",
                "os.makedirs(docs_path, exist_ok=True)\n",
                "\n",
                "label_counts = Counter(labels)\n",
                "sorted_counts = dict(sorted(label_counts.items()))\n",
                "\n",
                "class_names = {\n",
                "    0: 'none',\n",
                "    1: 'Center',\n",
                "    2: 'Donut',\n",
                "    3: 'Edge-Loc',\n",
                "    4: 'Edge-Ring',\n",
                "    5: 'Loc',\n",
                "    6: 'Near-full',\n",
                "    7: 'Random',\n",
                "    8: 'Scratch'\n",
                "}\n",
                "\n",
                "x = [class_names[k] for k in sorted_counts.keys()]\n",
                "y = list(sorted_counts.values())\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "bars = plt.bar(x, y)\n",
                "plt.title('Class Distribution (WM-811K)')\n",
                "plt.xlabel('Failure Type')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=45)\n",
                "\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:,}', ha='center', va='bottom', fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{docs_path}/class_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "defect_counts = [v for k, v in sorted_counts.items() if k != 0]\n",
                "if defect_counts:\n",
                "    imbalance_ratio = max(defect_counts) / min(defect_counts)\n",
                "    print(f\"Defect Imbalance Ratio (max/min): {imbalance_ratio:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ee37a11",
            "metadata": {
                "id": "sample_grid"
            },
            "outputs": [],
            "source": [
                "sample_indices = {}\n",
                "for idx, label in enumerate(labels):\n",
                "    if label not in sample_indices:\n",
                "        sample_indices[label] = idx\n",
                "    if len(sample_indices) == 9:\n",
                "        break\n",
                "\n",
                "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for class_id in range(9):\n",
                "    ax = axes[class_id]\n",
                "    if class_id in sample_indices:\n",
                "        img = images[sample_indices[class_id]]\n",
                "        ax.imshow(img, cmap='gray')\n",
                "        ax.set_title(class_names[class_id])\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{docs_path}/sample_defects.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dbc90f97",
            "metadata": {
                "id": "split_md"
            },
            "source": [
                "## 6. Stratified Train/Val/Test Split\n",
                "\n",
                "Splits the data while maintaining class proportions.\n",
                "- **Train**: 70%\n",
                "- **Validation**: 15%\n",
                "- **Test**: 15%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "454863ee",
            "metadata": {
                "id": "split_code"
            },
            "outputs": [],
            "source": [
                "import pickle\n",
                "import numpy as np\n",
                "from sklearn.model_selection import StratifiedShuffleSplit\n",
                "\n",
                "# Load existing processed dataset\n",
                "processed_path = '/content/drive/MyDrive/wafer-hackathon/data/processed/processed_dataset.pkl'\n",
                "\n",
                "with open(processed_path, 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "\n",
                "all_labels = np.array(data['labels'])\n",
                "all_indices = np.arange(len(all_labels))\n",
                "\n",
                "# First split: 70% train, 30% temp\n",
                "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
                "train_idx, temp_idx = next(sss1.split(all_indices, all_labels))\n",
                "\n",
                "# Second split: 50% val, 50% test from temp (15% each of total)\n",
                "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
                "val_idx, test_idx = next(sss2.split(temp_idx, all_labels[temp_idx]))\n",
                "val_idx = temp_idx[val_idx]\n",
                "test_idx = temp_idx[test_idx]\n",
                "\n",
                "print(f'Train size: {len(train_idx)}')\n",
                "print(f'Validation size: {len(val_idx)}')\n",
                "print(f'Test size: {len(test_idx)}')\n",
                "\n",
                "# Add indices to existing dictionary\n",
                "data['train_indices'] = train_idx\n",
                "data['val_indices'] = val_idx\n",
                "data['test_indices'] = test_idx\n",
                "\n",
                "# Re-save\n",
                "with open(processed_path, 'wb') as f:\n",
                "    pickle.dump(data, f)\n",
                "\n",
                "print('processed_dataset.pkl updated with split indices.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
